{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import h5py\n",
    "# sys.path.append(os.path.abspath(os.path.join(__file__, \"..\", \"..\")))\n",
    "from utils.utils import *\n",
    "    \n",
    "class NEXTQADataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        anno_path = '../nextqa/annotations_mc/train.csv',\n",
    "        mapper_path = '../nextqa/map_vid_vidorID.json',\n",
    "        video_path = \"../nextqa/videos\", \n",
    "        frame_path = \"../nextqa/frames_32\",\n",
    "        feature_path = \"../nextqa/vision_features/feats_wo_norm_32.h5\",\n",
    "        frame_count = 32\n",
    "    ):\n",
    "        \n",
    "        self.data = load_csv(anno_path)\n",
    "        self.mapper = load_json(mapper_path)\n",
    "        self.video_path = video_path\n",
    "        self.frame_path = frame_path\n",
    "        self.frame_count = frame_count\n",
    "        self.image_processor = image_transform(image_size=224)\n",
    "        self.image_features = h5py.File(feature_path, \"r\")\n",
    "\n",
    "        self.video_ids = []\n",
    "        self.videos = []\n",
    "        self.frames = []\n",
    "        self.questions = []\n",
    "        self.answers_option = []\n",
    "        self.answers_text = []\n",
    "        self.answers_ids = []\n",
    "        self.types = []\n",
    "        self.qids = []\n",
    "        self.options_a0 = []\n",
    "        self.options_a1 = []\n",
    "        self.options_a2 = []\n",
    "        self.options_a3 = []\n",
    "        self.options_a4 = []\n",
    "\n",
    "        for data in self.data:\n",
    "\n",
    "            self.video_ids.append(data['video'])\n",
    "            self.qids.append(data['qid'])\n",
    "            self.types.append(data['type'])\n",
    "            self.questions.append(data['question']+\"?\")\n",
    "            self.options_a0.append(data['a0'])\n",
    "            self.options_a1.append(data['a1'])\n",
    "            self.options_a2.append(data['a2'])\n",
    "            self.options_a3.append(data['a3'])\n",
    "            self.options_a4.append(data['a4'])\n",
    "\n",
    "            self.answers_ids.append(data['answer'])\n",
    "            self.answers_text.append(data[f\"a{str(data['answer'])}\"] )\n",
    "            self.answers_option.append([\"A\", \"B\", \"C\", \"D\", \"E\"][data['answer']])\n",
    "            self.videos.append(self.video_path + f\"/{self.mapper[str(data['video'])]}.mp4\")\n",
    "            self.frames.append(self.frame_path +f\"/{str(data['video'])}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the length of dataframe\"\"\"\n",
    "        return len(self.video_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
    "        video_id = str(self.video_ids[index])\n",
    "        qid = str(self.qids[index])\n",
    "        type = str(self.types[index])\n",
    "        question = str(self.questions[index])\n",
    "        option_a0 = str(self.options_a0[index])\n",
    "        option_a1 = str(self.options_a1[index])\n",
    "        option_a2 = str(self.options_a2[index])\n",
    "        option_a3 = str(self.options_a3[index])\n",
    "        option_a4 = str(self.options_a4[index])\n",
    "        answer_id = self.answers_ids[index]\n",
    "        answer_text = str(self.answers_text[index])\n",
    "        answer_option = str(self.answers_option[index])\n",
    "\n",
    "        frame_files = os.listdir(str(self.frames[index]))\n",
    "        frame_files = sorted(frame_files, key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "        frame_files = get_frames(frame_files, self.frame_count)\n",
    "\n",
    "        # frame_features = []\n",
    "        # for i in range(len(frame_files)):\n",
    "        #     frame_features.append(torch.from_numpy(self.image_features[f\"{video_id}_{frame_files[i].replace('.jpg','')}\"][:]))\n",
    "        # frame_features = torch.stack(frame_features, dim=0) # [frame_count, 257, 1408]\n",
    "        \n",
    "        return {\n",
    "                \"video_ids\": video_id,\n",
    "                \"qids\": qid,\n",
    "                \"types\": type,\n",
    "\n",
    "                # \"frame_features\": frame_features,\n",
    "                \"frame_files\": frame_files,\n",
    "\n",
    "                \"questions\": question,\n",
    "                \"options_a0\": option_a0,\n",
    "                \"options_a1\": option_a1,\n",
    "                \"options_a2\": option_a2,\n",
    "                \"options_a3\": option_a3,\n",
    "                \"options_a4\": option_a4,\n",
    "                \"answers_id\": answer_id,\n",
    "                \"answers_text\": answer_text,\n",
    "                \"answers\": answer_option,\n",
    "\n",
    "            }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"각 QA pair 를 plot 으로 변환해서 ./temp/ 에 저장 (이미지 시퀀스 + QA 텍스트박스)\"\"\"\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tqdm.auto as tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Create a temporary directory to save plots\n",
    "os.makedirs('./temp', exist_ok=True)\n",
    "\n",
    "# train_dataset = NEXTQADataset(anno_path='../nextqa/annotations_mc/train.csv', frame_count=32)\n",
    "train_dataset = NEXTQADataset(anno_path='../nextqa/annotations_mc/test.csv', frame_count=32)\n",
    "image_transform = transforms.Compose([\n",
    "        Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "        CenterCrop(224),\n",
    "    ])\n",
    "\n",
    "def export_to_plot(sample):\n",
    "    video_id = sample['video_ids']\n",
    "    qa_text = (f\"video_ids: {video_id}  qids: {sample['qids']}  types: {sample['types']}\\n\"\n",
    "            f\"Question: {sample['questions']}\\n\"\n",
    "            f\"Options: \\n\"\n",
    "            f\"  A: {sample['options_a0']}\\n\"\n",
    "            f\"  B: {sample['options_a1']}\\n\"\n",
    "            f\"  C: {sample['options_a2']}\\n\"\n",
    "            f\"  D: {sample['options_a3']}\\n\"\n",
    "            f\"  E: {sample['options_a4']}\\n\"\n",
    "            f\"Answer: {sample['answers']}\")\n",
    "\n",
    "    # Create a plot for the video frames\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(20, 12))  # Increase height to add space for text\n",
    "    axes = axes.flatten()\n",
    "    for ax, frame in zip(axes, sample['frame_files']):\n",
    "        img = Image.open(f\"../nextqa/frames_32/{video_id}/{frame}\")\n",
    "        img = image_transform(img)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        # Add the filename as a title\n",
    "        ax.set_title(frame, fontsize=8)\n",
    "\n",
    "    # Add a textbox with info at the top left corner\n",
    "    plt.gcf().text(0.02, 0.98, qa_text, fontsize=12, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    # Save the plot to the temporary directory\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to make space for the text\n",
    "    plt.savefig(f'./temp/{video_id}_{sample[\"qids\"]}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# 병렬 처리\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "num_workers = 16\n",
    "temp_paths = Parallel(num_workers)(delayed(export_to_plot)(x) for x in tqdm.tqdm(train_dataset, total=len(train_dataset)))\n",
    "\n",
    "print(\"Plots saved to ./temp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"fiftyone 시각화\"\"\"\n",
    "\n",
    "\n",
    "import fiftyone as fo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "# # Fiftyone Dataset 생성\n",
    "dataset = fo.Dataset(\"nextqa\", persistent=True, overwrite=True)\n",
    "# dataset = fo.load_dataset(\"nextqa\")\n",
    "\n",
    "# train_dataset = NEXTQADataset(anno_path='../nextqa/annotations_mc/train.csv', frame_count=32)\n",
    "test_dataset = NEXTQADataset(anno_path='../nextqa/annotations_mc/test.csv', frame_count=32)\n",
    "acc_records = torch.load('acc_records.pth', map_location=\"cpu\")\n",
    "acc_records = pd.DataFrame(acc_records).set_index(['video_id', 'qid'])\n",
    "\n",
    "\n",
    "sample_list = []\n",
    "for x in tqdm.tqdm(test_dataset):\n",
    "    video_id = x['video_ids']\n",
    "    types = x['types']\n",
    "    qids = x['qids']\n",
    "    file = f'./temp/{video_id}_{qids}.png'\n",
    "    label = acc_records.loc[video_id, qids]['label']\n",
    "    pred = acc_records.loc[video_id, qids]['pred']\n",
    "    confidence = acc_records.loc[video_id, qids]['sequences_scores'].item()\n",
    "\n",
    "    sample = fo.Sample(filepath=file)\n",
    "    sample['label'] = fo.Classification(label=label)\n",
    "    sample['pred'] = fo.Classification(label=pred, confidence=confidence)\n",
    "    sample[\"split\"] = fo.Classification(label=\"train\")\n",
    "    sample['video_ids'] = int(video_id)\n",
    "    sample['qids'] = int(qids)\n",
    "    sample['types'] = fo.Classification(label=types)\n",
    "    sample_list.append(sample)\n",
    "\n",
    "_ = dataset.add_samples(sample_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"FiftyOne 앱 실행\"\"\"\n",
    "\n",
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset(\"nextqa\")\n",
    "\n",
    "# VSCode port forwarding 으로 5152를 포워딩한 후 웹브라우저에서 http://localhost:5152/ 접속\n",
    "session = fo.launch_app(dataset, auto=False, port=5152)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = dataset.match(fo.ViewField(\"label.label\") != fo.ViewField(\"pred.label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
